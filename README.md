# End-to-End Automated Fraud Risk Pipeline using PySpark and Airflow on Google Cloud

Production-grade, serverless pipeline that transforms raw JSON transactions into analytics-ready fraud insights‚Äîprocessing daily files in under 10 minutes with 100% unit test coverage.

‚ñ∂Ô∏è **Watch the Full Demo (Code, UI, Results)** `[YOUR_LOOM_DEMO_LINK]` | üîó **View the Project Architecture** `[YOUR_ARCHITECTURE_LINK]` | üìä **Jump to Results & Validation** `[#14-execution--results]`

## 1. TL;DR for Recruiters

* **Business Impact:** Automates the daily processing of raw transaction logs into an analysis-ready, enriched BigQuery table. Enables near real-time fraud risk assessment, replacing slow, manual, and error-prone analysis.
* **Technical Impact:** Demonstrates a modern, serverless GCP data stack. The pipeline is fully automated via Airflow, scalable via Dataproc Serverless, and reliable due to CI/CD (GitHub Actions) and robust unit testing (PyTest).
* **Core Skills:**
    * **Data Processing:** PySpark (DataFrames, SQL, transformations, joins).
    * **Orchestration:** Apache Airflow (GCP Composer, DAGs, Operators, Sensors).
    * **Cloud Platform:** GCP (Dataproc, BigQuery, GCS, Composer).
    * **DevOps/DataOps:** CI/CD with GitHub Actions, Unit Testing with PyTest.
    * **Language:** Python.

## 2. Quick Start Guide

üéØ **Recruiters & Hiring Managers (The "Why" & "Impact"):**
* **[TL;DR for Recruiters](#1-tldr-for-recruiters)**
* **[Impact at a Glance](#3-impact-at-a-glance)**
* **[Business Impact & Use Cases](#7-business-impact--use-cases)**
